{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries we are planning to use.\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Import dataset.\n",
    "cc_dataframe = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "# See how many rows and columns\n",
    "print(f\"Rows: {cc_dataframe.shape[0]} and Columns: {cc_dataframe.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the column labels\n",
    "cc_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze first and last 10 values of data to see what we have to work with.\n",
    "cc_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_dataframe.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All values are continious numbers except class which is discrete.\n",
    "# Lets look for missing data, larger missing count will be on top\n",
    "cc_dataframe.isnull().sum().sort_values(ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# No missing data, great!\n",
    "# Time amount, and class are original values, other values are PCA transformation due to confidentiality.\n",
    "# First I will explore time, amount, and class because I know what they represent.\n",
    "cc_dataframe[[\"Time\",\"Amount\",\"Class\"]].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time and amount are floats, class is integer.\n",
    "cc_dataframe[[\"Time\",\"Amount\",\"Class\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Time is incremental starting at 0 and going up to 172792 seconds.\n",
    "# The class field data is integer represented by 0 as non fraud, and 1 as fraud.\n",
    "# Only 0.17% of all transactions are fraud, which is heavily unbalanced.\n",
    "# We can confirm that by looking at the count of each class value\n",
    "cc_dataframe.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column to interpret as hour\n",
    "hourly_dataframe = cc_dataframe.copy()\n",
    "hourly_dataframe['Hour'] = hourly_dataframe['Time'].apply(lambda x: np.floor(x / 3600))\n",
    "\n",
    "# Lets look at the amount of transactions for each hour described as fraudulent or not.\n",
    "sns.lineplot(data=hourly_dataframe, x=\"Hour\", y=\"Amount\", hue=\"Class\")\n",
    "plt.suptitle(\"Transaction amount per Hour\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the volume of transactions for each hour described as fraudulent or not.\n",
    "hourly_agregate = hourly_dataframe.groupby(['Hour','Class'])['Class'].aggregate(['count', 'max']).reset_index()\n",
    "hourly_dataframe_agregate = pd.DataFrame(hourly_agregate)\n",
    "hourly_dataframe_agregate.columns = ['Hour', 'Class', 'Transactions', 'Max']\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\n",
    "s = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Transactions\", data=hourly_dataframe_agregate.loc[hourly_dataframe_agregate.Class==0])\n",
    "s = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Transactions\", data=hourly_dataframe_agregate.loc[hourly_dataframe_agregate.Class==1], color=\"orange\")\n",
    "plt.suptitle(\"Transactions per Hour\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the volume of transactions for each hour described as fraudulent or not.\n",
    "time_amount_aggregate = cc_dataframe.groupby(['Time','Class'])['Amount'].aggregate(['min', 'max', 'count', 'sum', 'mean', 'median', 'var']).reset_index()\n",
    "amount_over_time_dataframe = pd.DataFrame(time_amount_aggregate)\n",
    "amount_over_time_dataframe.columns = ['Time', 'Class', 'Min', 'Max', 'Transactions', 'Sum', 'Mean', 'Median', 'Var']\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\n",
    "s = sns.lineplot(ax = ax1, x=\"Time\", y=\"Max\", data=amount_over_time_dataframe.loc[amount_over_time_dataframe.Class==0])\n",
    "s = sns.lineplot(ax = ax2, x=\"Time\", y=\"Max\", data=amount_over_time_dataframe.loc[amount_over_time_dataframe.Class==1], color=\"orange\")\n",
    "plt.suptitle(\"Largest Transaction per Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a clear relation between the hour in which transaction occurs\n",
    "# and the amount for a fraudulent transaction.\n",
    "# Lets see if there are features which have direct correlation between each other.\n",
    "plt.figure(figsize = (7,7))\n",
    "correlation_dataframe = cc_dataframe.corr()\n",
    "sns.heatmap(correlation_dataframe,xticklabels=correlation_dataframe.columns,yticklabels=correlation_dataframe.columns,linewidths=.1,cmap=\"Reds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data to fit between 0-1 floating point values only\n",
    "for column in ['Time','Amount']:\n",
    "    cc_dataframe[column] = cc_dataframe[column]  / cc_dataframe[column].abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of regular kfold, we are using stratified to make sure distribution of classes is equal for the unbalanced dataset.\n",
    "five_fold = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "# Next we will create differente variations of data to test\n",
    "# Separate dataset between valid and invalid transactions\n",
    "valid_transactions_dataframe = cc_dataframe[cc_dataframe.Class == 0]\n",
    "invalid_transactions_dataframe = cc_dataframe[cc_dataframe.Class == 1]\n",
    "\n",
    "# Randomly trim invalid transaction so data is not unbalanced\n",
    "trimmed_valid_transactions_dataframe = resample(valid_transactions_dataframe, n_samples=(invalid_transactions_dataframe.shape[0]*3), random_state=7)\n",
    "\n",
    "# Merge the trimmed valid transactions with the invalid transactions\n",
    "trimmed_cc_dataframe = pd.concat([trimmed_valid_transactions_dataframe,invalid_transactions_dataframe],axis=0)\n",
    "\n",
    "# Extract our X and y dataframes to run cross validation score with\n",
    "X_trimmed = trimmed_cc_dataframe.drop(columns='Class')\n",
    "y_trimmed = trimmed_cc_dataframe.Class\n",
    "\n",
    "X_original = cc_dataframe.drop(columns='Class')\n",
    "y_original = cc_dataframe.Class\n",
    "\n",
    "# Use SMOTE library to oversample the invalid transactions\n",
    "X_smote, y_smote = SMOTE().fit_resample(X_original, y_original)\n",
    "\n",
    "\n",
    "# Create data structure to iterate over\n",
    "X_y_dictionary = {\n",
    "  'original': [X_original,y_original],\n",
    "  'trimmed': [X_trimmed,y_trimmed],\n",
    "  'SMOTE': [X_smote,y_smote]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "RF = RandomForestClassifier()\n",
    "results = {}\n",
    "for trial_type, X_y in X_y_dictionary.items():\n",
    "  scores = cross_val_score(RF, X_y[0], X_y[1], scoring='accuracy', cv=five_fold, n_jobs=-1)\n",
    "  accuracy = str(round(((sum(scores) / len(scores)) * 100), 2)) + '%'\n",
    "  results.update({trial_type: accuracy})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splits data into train and test.\n",
    "X_original_train, X_original_test, y_original_train, y_original_test = train_test_split(X_original, y_original, test_size=0.25, random_state=7, stratify=y_original)\n",
    "X_trimmed_train, X_trimmed_test, y_trimmed_train, y_trimmed_test = train_test_split(X_trimmed, y_trimmed, test_size=0.25, random_state=7, stratify=y_trimmed)\n",
    "X_smote_train, X_smote_test, y_smote_train, y_smote_test = train_test_split(X_smote, y_smote, test_size=0.25, random_state=7, stratify=y_smote)\n",
    "\n",
    "# Merge test and data used to validate prediction\n",
    "original_test_merged = X_original_test.join(y_original_test)\n",
    "trimmed_test_merged = X_trimmed_test.join(y_trimmed_test)\n",
    "smote_test_merged = X_smote_test.join(y_smote_test)\n",
    "\n",
    "# This test dataset is used run predictions and validate predictions.\n",
    "test_dataset = {\n",
    "  'original': {\n",
    "    'valid': original_test_merged[original_test_merged.Class == 0].drop(columns='Class'),\n",
    "    'invalid': original_test_merged[original_test_merged.Class == 1].drop(columns='Class'),\n",
    "    'valid_count': original_test_merged.Class.value_counts()[0],\n",
    "    'invalid_count': original_test_merged.Class.value_counts()[1]\n",
    "    },\n",
    "  'trimmed': {\n",
    "    'valid': trimmed_test_merged[trimmed_test_merged.Class == 0].drop(columns='Class'),\n",
    "    'invalid': trimmed_test_merged[trimmed_test_merged.Class == 1].drop(columns='Class'),\n",
    "    'valid_count': trimmed_test_merged.Class.value_counts()[0],\n",
    "    'invalid_count': trimmed_test_merged.Class.value_counts()[1]\n",
    "    },\n",
    "  'SMOTE': {\n",
    "    'valid': smote_test_merged[smote_test_merged.Class == 0].drop(columns='Class'),\n",
    "    'invalid': smote_test_merged[smote_test_merged.Class == 1].drop(columns='Class'),\n",
    "    'valid_count': smote_test_merged.Class.value_counts()[0],\n",
    "    'invalid_count': smote_test_merged.Class.value_counts()[1]\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a structure with the different trials fitted to the classifier.\n",
    "classifiers = {\n",
    "    'original': RF.fit(X_original_train,y_original_train),\n",
    "    'trimmed': RF.fit(X_trimmed_train,y_trimmed_train),\n",
    "    'SMOTE': RF.fit(X_smote_train, y_smote_train),\n",
    " }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the data against the respective testing set.\n",
    "prediction_results = {}\n",
    "for classifier_type, classifier_object in classifiers.items():\n",
    "  # Predict against test dataset consisting of only valid or invalid transactions.\n",
    "  # Then compare success rate against that particular set.\n",
    "  valid_predictions = Counter(classifier_object.predict(test_dataset[classifier_type]['valid']))\n",
    "  invalid_predictions = Counter(classifier_object.predict(test_dataset[classifier_type]['invalid']))\n",
    "  # Prediction 0 is valid, 1 is invalid. Calculate rate of valid and invalid against the actual count.\n",
    "  if valid_predictions[0]:\n",
    "    valid_rate = str(round((valid_predictions[0] / test_dataset[classifier_type]['valid_count']) * 100,2)) + '%'\n",
    "  else:\n",
    "    valid_rate = '0%'\n",
    "    \n",
    "  if invalid_predictions[1]:\n",
    "    invalid_rate = str(round((invalid_predictions[1] / test_dataset[classifier_type]['invalid_count']) * 100,2)) + '%'\n",
    "  else:\n",
    "    invalid_rate = '0%'\n",
    "  # Write results\n",
    "  prediction_results.update({\n",
    "    classifier_type: {\n",
    "    'valid_accuracy': valid_rate,\n",
    "    'invalid_accuracy': invalid_rate\n",
    "  }})\n",
    "  \n",
    "\n",
    "from pprint import pprint\n",
    "pprint(prediction_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the data against the respective testing set.\n",
    "prediction_results = {}\n",
    "\n",
    "classifier_object = classifiers['trimmed']\n",
    "test_against = 'original'\n",
    "\n",
    "valid_predictions = Counter(classifier_object.predict(test_dataset[test_against]['valid']))\n",
    "invalid_predictions = Counter(classifier_object.predict(test_dataset[test_against]['invalid']))\n",
    "# Prediction 0 is valid, 1 is invalid. Calculate rate of valid and invalid against the actual count.\n",
    "if valid_predictions[0]:\n",
    "  valid_rate = str(round((valid_predictions[0] / test_dataset[test_against]['valid_count']) * 100,2)) + '%'\n",
    "else:\n",
    "  valid_rate = '0%'\n",
    "  \n",
    "if invalid_predictions[1]:\n",
    "  invalid_rate = str(round((invalid_predictions[1] / test_dataset[test_against]['invalid_count']) * 100,2)) + '%'\n",
    "else:\n",
    "  invalid_rate = '0%'\n",
    "# Write results\n",
    "prediction_results.update({\n",
    "  test_against: {\n",
    "  'valid_accuracy': valid_rate,\n",
    "  'invalid_accuracy': invalid_rate\n",
    "}})\n",
    "  \n",
    "\n",
    "from pprint import pprint\n",
    "pprint(prediction_results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
